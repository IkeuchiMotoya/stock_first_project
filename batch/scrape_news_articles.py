import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import pandas as pd
import os
import chardet

# ================================
# ğŸ”§ è¨­å®š
# ================================
DAYS_BACK = 3
JST = timezone(timedelta(hours=9))
END_DATE = datetime.now(JST)
START_DATE = END_DATE - timedelta(days=DAYS_BACK)

CSV_PATHS = [
    r'C:\Users\pumpk\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\æ ªå¼\csv\ä¿æœ‰éŠ˜æŸ„\ä¿æœ‰éŠ˜æŸ„_ä¿¡ç”¨.csv',
    r'C:\Users\pumpk\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\æ ªå¼\csv\ä¿æœ‰éŠ˜æŸ„\ä¿æœ‰éŠ˜æŸ„_ç¾ç‰©.csv'
]

OUTPUT_PATH = r'C:\Users\pumpk\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\æ ªå¼\kabutan_news_æœŸé–“æŒ‡å®š_çµ±åˆç‰ˆ.xlsx'

# ================================
# ğŸ“¥ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆè‡ªå‹•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
# ================================
def load_stock_csv(filepath):
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
    with open(filepath, 'rb') as f:
        encoding = chardet.detect(f.read())['encoding']
    df = pd.read_csv(filepath, encoding=encoding, dtype=str)
    df = df.rename(columns=lambda x: x.strip())
    return df[["ã‚³ãƒ¼ãƒ‰", "éŠ˜æŸ„å"]].dropna()

# ================================
# ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–¢æ•°
# ================================
def fetch_news_in_range(stock_code, stock_name, start_date, end_date):
    url = f'https://kabutan.jp/stock/news?code={stock_code}'
    resp = requests.get(url)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')
    news_rows = soup.select('table.s_news_list tr')
    collected_news = []

    for row in news_rows:
        time_td = row.find('td', class_='news_time')
        if not time_td:
            continue
        time_text = time_td.text.strip()
        try:
            news_date = datetime.strptime(time_text[:8], '%y/%m/%d').replace(tzinfo=JST)
        except ValueError:
            continue
        if start_date <= news_date <= end_date:
            title_td = row.find_all('td')[-1]
            a_tag = title_td.find('a')
            if a_tag:
                title = a_tag.text.strip()
                href = a_tag['href']
                link = href if href.startswith('http') else 'https://kabutan.jp' + href
                collected_news.append({
                    'éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰': stock_code,
                    'éŠ˜æŸ„å': stock_name,
                    'æ™‚é–“': time_text,
                    'ã‚¿ã‚¤ãƒˆãƒ«': title,
                    'URL': link
                })
    return collected_news

# ================================
# ğŸ’¾ Excelä¿å­˜é–¢æ•°
# ================================
def save_to_excel(news_data, filepath):
    df = pd.DataFrame(news_data)
    df.to_excel(filepath, index=False)
    os.startfile(filepath)

# ================================
# ğŸ” ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ================================
def main():
    all_news = []

    for path in CSV_PATHS:
        try:
            stock_df = load_stock_csv(path)
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼š{path} â†’ {e}")
            continue

        for _, row in stock_df.iterrows():
            code = row["ã‚³ãƒ¼ãƒ‰"].zfill(4)
            name = row["éŠ˜æŸ„å"]
            try:
                news = fetch_news_in_range(code, name, START_DATE, END_DATE)
                all_news.extend(news)
            except Exception as e:
                print(f"[{code}] {name} ã®å–å¾—å¤±æ•—: {e}")

    if all_news:
        save_to_excel(all_news, OUTPUT_PATH)
        print(f"âœ… {len(all_news)} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’Excelã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    else:
        print("ğŸ” æŒ‡å®šæœŸé–“ã«è©²å½“ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

if __name__ == '__main__':
    main()
